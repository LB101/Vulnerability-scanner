import urllib.request
import urllib.error

def check_robots_txt(url):
    robots_url = url.rstrip('/') + '/robots.txt'
    try:
        urllib.request.urlopen(robots_url)
        print("robots.txt exists for", url)
    except urllib.error.HTTPError as e:
        if e.code == 404:
            print("robots.txt does not exist for", url)
        else:
            print("An HTTP error occurred while checking robots.txt for {}: {}".format(url, e))
    except urllib.error.URLError as e:
        print("Failed to retrieve robots.txt for {}: {}".format(url, e.reason))
    except Exception as e:
        print("An error occurred while checking robots.txt for {}: {}".format(url, e))

# Example usage
url = "https://ksa.com"
check_robots_txt(url)
