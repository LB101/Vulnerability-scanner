import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import time
import sys

visited_urls = set()

def extract_links(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    links = []

    for link in soup.find_all('a', href=True):
        href = link.get('href')
        if href.startswith('http'):
            absolute_url = href
        else:
            absolute_url = urljoin(url, href)
        if urlparse(absolute_url).netloc == urlparse(url).netloc:  # Ensure link is within the same domain
            links.append(absolute_url)

    return links

def crawl(url, start_time, duration):
    global visited_urls

    if time.time() - start_time >= duration:
        return

    if url in visited_urls:
        return
    visited_urls.add(url)

    print("Crawling:", url)
    links = extract_links(url)
    links_with_params = [link for link in links if '?' in link]
    
    if links_with_params:
        # print("Links with parameters:")
        for link in links_with_params:
            print(link)

    for link in links:
        crawl(link, start_time, duration)

def main():
    url =  sys.argv[1] # passing value from php
    # url = "https://0x00sec.org/"
    start_time = time.time()
    duration = 20  # seconds

    crawl(url, start_time, duration)

if __name__ == "__main__":
    main()
